06/03/2022 12:57:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 2, distributed training: False, 16-bits training: False
06/03/2022 12:57:12 - INFO - __main__ -   Training/evaluation parameters Namespace(MRR_num=5, PAD_ID=0, adam_epsilon=1e-08, ans_size=185, block_size=185, cache_dir='', config_name='bert-base-chinese', device=device(type='cuda'), do_eval=False, do_lower_case=False, do_test=True, do_train=False, epoch=300, eval_all_checkpoints=False, eval_batch_size=64, eval_data_file='None', evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path=None, model_type='chinese_bert', n_gpu=2, no_cuda=False, num_train_epochs=1.0, output_dir='./run_bert-base-chinese_chaizi', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=32, per_gpu_train_batch_size=32, save_steps=50, save_total_limit=None, seed=123456, server_ip='', server_port='', start_epoch=0, start_step=0, test_data_file='../../dataset/test.txt.fake.csv.for_seq_only_chaizi.json', tokenizer_name='bert-base-chinese', train_batch_size=64, train_data_file='None', warmup_steps=0, weight_decay=0.0, with_qq=True)
06/03/2022 12:57:28 - INFO - __main__ -   load model from ./run_bert-base-chinese_chaizi/checkpoint-for-final/model.bin
06/03/2022 12:57:29 - INFO - __main__ -   ***** Running Test *****
06/03/2022 12:57:29 - INFO - __main__ -     Num examples = 5413
06/03/2022 12:57:29 - INFO - __main__ -     Batch size = 64
/home/zhangkechi/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 0/5413 [00:00<?, ?it/s]100%|██████████| 5413/5413 [00:00<00:00, 119906.88it/s]
